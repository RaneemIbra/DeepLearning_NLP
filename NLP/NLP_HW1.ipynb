{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['13_ptm_532058.docx', '13_ptm_532066.docx', '13_ptm_532240.docx', '13_ptm_532389.docx', '14_ptm_532484.docx', '14_ptm_532608.docx', '14_ptm_532731.docx', '15_ptm_532756.docx', '15_ptm_532855.docx', '15_ptm_533086.docx', '15_ptv_490845.docx', '15_ptv_490916.docx', '15_ptv_494321.docx', '15_ptv_494780.docx', '15_ptv_495206.docx', '15_ptv_495295.docx', '15_ptv_496915.docx', '15_ptv_496944.docx', '15_ptv_498215.docx', '16_ptm_128954.docx', '16_ptm_128968.docx', '16_ptm_129080.docx', '16_ptm_129137.docx', '16_ptm_129202.docx', '16_ptm_533215.docx', '16_ptm_533607.docx', '16_ptv_386758.docx', '16_ptv_386822.docx', '16_ptv_386833.docx', '16_ptv_489839.docx', '16_ptv_491962.docx', '16_ptv_493376.docx', '16_ptv_499021.docx', '16_ptv_499045.docx', '16_ptv_548123.docx', '16_ptv_548530.docx', '16_ptv_549100.docx', '16_ptv_572718.docx', '16_ptv_577443.docx', '16_ptv_577758.docx', '16_ptv_581836.docx', '16_ptv_71595.docx', '17_ptm_129748.docx', '17_ptm_533398.docx', '17_ptm_533401.docx', '17_ptm_533539.docx', '17_ptv_132014.docx', '17_ptv_132523.docx', '17_ptv_132714.docx', '17_ptv_133151.docx', '17_ptv_133454.docx', '17_ptv_133749.docx', '17_ptv_134006.docx', '17_ptv_134507.docx', '17_ptv_135103.docx', '17_ptv_135556.docx', '17_ptv_136809.docx', '17_ptv_137100.docx', '18_ptv_138917.docx', '18_ptv_139021.docx', '18_ptv_139299.docx', '18_ptv_139592.docx', '18_ptv_139917.docx', '18_ptv_139933.docx', '18_ptv_165084..docx', '18_ptv_165095..docx', '18_ptv_166075..docx', '19_ptm_304163.docx', '19_ptv_232326.docx', '19_ptv_253687.docx', '19_ptv_262672.docx', '19_ptv_266962.docx', '19_ptv_279660.docx', '19_ptv_302840.docx', '20_ptm_313902.docx', '20_ptm_341723.docx', '20_ptm_394541.docx', '20_ptm_487089.docx', '20_ptv_311936.docx', '20_ptv_320584.docx', '20_ptv_341020.docx', '20_ptv_341230.docx', '20_ptv_370910.docx', '20_ptv_387379.docx', '20_ptv_397418.docx', '20_ptv_488037.docx', '20_ptv_490139.docx', '20_ptv_494974.docx', '20_ptv_519812.docx', '23_ptv_578475.docx', '23_ptv_582824.docx', '23_ptv_598323 - Copy.docx', '23_ptv_598323.docx', '23_ptv_598723.docx', '23_ptv_599659.docx', '23_ptv_600338.docx', '25_ptv_1219728.docx', '25_ptv_1457545.docx', '25_ptv_3841247.docx', 'for_test_23_ptv_585004.docx']\n",
      "JSONL file saved at: C:/Users/Alpha/Downloads/knesset_protocols/protocol_for_hw1/protocol_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "from docx import Document\n",
    "\n",
    "# function to split text into sentences by seeing the punctuation and splitting from there\n",
    "def divideToSentences(text):\n",
    "    # we create a list to store the sentences and initialize an empty sentence to add to it the sentences that we find\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "    # loop over the whole text and gather the sentences\n",
    "    for char in text:\n",
    "        # we add all the chars to the sentence\n",
    "        sentence += char\n",
    "        # if the char is a punctuation that ends the sentence then we finish the sentence\n",
    "        if char == \".\" or char == \"!\" or char == \"?\":\n",
    "            # we check if the sentence has a meaning and not only a single char then we add it\n",
    "            if len(sentence.strip()) > 1:\n",
    "                # we add the sentence to the list, and we use strip to avoid white spaces\n",
    "                sentences.append(sentence.strip())\n",
    "                # init the sentence to blank again to avoid accumulation \n",
    "                sentence = \"\"\n",
    "    # after we finish looping over the text we add any remaining text and then return the list\n",
    "    if sentence.strip():\n",
    "        sentences.append(sentence.strip())\n",
    "    return sentences\n",
    "\n",
    "# function to check if a sentence is valid\n",
    "def ValidSentence(sentence):\n",
    "    # check if the sentence contains Hebrew characters, the regex is the unicode for hebrew letters\n",
    "    containsHebrew = False\n",
    "    for char in sentence:\n",
    "        if \"\\u0590\" <= char <= \"\\u05FF\":\n",
    "            containsHebrew = True\n",
    "            break\n",
    "    if not containsHebrew:\n",
    "        return False\n",
    "    # if the sentence doesn't have any meaningful text like special chars only then we want to not include it\n",
    "    words = re.split(r'\\s+', sentence.strip())\n",
    "    if all(re.fullmatch(r\"[^\\w\\u0590-\\u05FF]+\", word) for word in words):\n",
    "        return False\n",
    "    # if the sentence has placeholders we also want to avoid it\n",
    "    if \"...\" in sentence or \"---\" in sentence:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# function to tokenize a sentence into words and symbols\n",
    "def Tokenize(sentence):\n",
    "    # a list that will hold all the tokens\n",
    "    tokens = []\n",
    "    # split the sentence into words from the whitespaces\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        currentToken = \"\"\n",
    "        for char in word:\n",
    "            # check if the char is a letter or a number and if yes then add it to the token\n",
    "            if char.isalnum():\n",
    "                currentToken += char\n",
    "            # if we encounter a special char\n",
    "            else:\n",
    "                # check if the token is not empty\n",
    "                if currentToken:\n",
    "                    # add the token to the list\n",
    "                    tokens.append(currentToken)\n",
    "                    # then we reset the current token\n",
    "                    currentToken = \"\"\n",
    "                # we add the punctuation or the symbol that is left as a separate token\n",
    "                tokens.append(char)\n",
    "\n",
    "        # if there is any token after the loop, we add it to the tokens list.\n",
    "        if currentToken:\n",
    "            tokens.append(currentToken)\n",
    "    return tokens\n",
    "\n",
    "# main processing function, where almost all the requirements happen (explained in each line)\n",
    "# the function takes the input folder where all the docx file are, and also where the output file will be stored\n",
    "def workOnFilesFunc(inputFolder, outputFile):\n",
    "    # we read the protocol files to start processing them\n",
    "    files = []\n",
    "    for file_name in os.listdir(inputFolder):\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            files.append(file_name)\n",
    "    # for debugging to check if we are reading the correct files\n",
    "    print(f\"Found protocol files: {files}\")\n",
    "    # init an empty list for the data that we will store in the jsonl file\n",
    "    jsonlList = []\n",
    "    # loop over the files to start the processing\n",
    "    for file in files:\n",
    "        # we use regex here to extract the kenest number from the docx title\n",
    "        match = re.search(r'(\\d+)_pt', file)\n",
    "        # extract knesset number and if we didn't find a number then as a fallback we assign -1\n",
    "        knessetNumber = int(match.group(1)) if match else -1\n",
    "        # the following if else statements check if the file is for a plenary or a committee and as a fallback we assign undefined\n",
    "        if \"ptm\" in file:\n",
    "            protocolType = \"plenary\"\n",
    "        elif \"ptv\" in file:\n",
    "            protocolType = \"committee\"\n",
    "        else:\n",
    "            protocolType = \"undefined\"\n",
    "        # we start with a none value for the protocol number\n",
    "        protocolNum = None\n",
    "        # we use a try except block as proposed in the hw document to start processing\n",
    "        try:\n",
    "            # we create the path for the docx and we open it using the Document func\n",
    "            docPath = os.path.join(inputFolder, file)\n",
    "            doc = Document(docPath)\n",
    "            # start by extracting the protocol number from the first 10 paragraphs (as per our choice)\n",
    "            for paragraph in doc.paragraphs[:10]:\n",
    "                # search for the protocol number using the text in hebrew\n",
    "                foundNum = re.search(r\"פרוטוקול מס'? (\\d+)\", paragraph.text)\n",
    "                # if the found number isn't none then we assign it to the protocol number and break out of the loop to stop the search for the number\n",
    "                if foundNum:\n",
    "                    protocolNum = int(foundNum.group(1))\n",
    "                    break\n",
    "            # if we didn't find a protocol number then we set it to -1 as a fall back as requested\n",
    "            if protocolNum is None:\n",
    "                protocolNum = -1\n",
    "            # we define a last that will benifit us to attribute texts that belongs to no one from the context\n",
    "            lastSpeaker = None\n",
    "            # loop over all the paragraphs in the document\n",
    "            for paragraph in doc.paragraphs:\n",
    "                # remove the white spaces\n",
    "                text = paragraph.text.strip()\n",
    "                # if the paragraph is empty then we skip it\n",
    "                if not text:\n",
    "                    continue\n",
    "                # we used regex again to find the speaker name by checking for a name followed by :\n",
    "                speakerFound = re.match(r\"^([\\u0590-\\u05FF\\w\\s\\(\\)]+):\", text)\n",
    "                if speakerFound:\n",
    "                    #we extract the raw name\n",
    "                    rawName = speakerFound.group(1)\n",
    "                    # here we remove any additional information like titles for example\n",
    "                    name = re.sub(r\"\\s*\\(.*?\\)\", \"\", rawName).strip()\n",
    "                    # we extract the text that was said after the :\n",
    "                    spokenText = text[len(speakerFound.group(0)):].strip()\n",
    "                    # then we assign the name to the last speaker so that we can attribute what is said after\n",
    "                    lastSpeaker = name\n",
    "                    # call the divideToSentences function to divide the text\n",
    "                    sentences = divideToSentences(spokenText)\n",
    "                    # loop over the sentences to process them further\n",
    "                    for sentence in sentences:\n",
    "                        # check if the sentence is valid according to the requirements\n",
    "                        if ValidSentence(sentence):\n",
    "                            # if the sentence is valid then we tokenize it\n",
    "                            tokens = Tokenize(sentence)\n",
    "                            # now we check if the token is longer than 4 (or equal)\n",
    "                            if len(tokens) >= 4:\n",
    "                                # if the token satisfies then we want to store it in the list\n",
    "                                jsonlList.append({\n",
    "                                    \"protocol_name\": file,\n",
    "                                    \"knesset_number\": knessetNumber,\n",
    "                                    \"protocol_type\": protocolType,\n",
    "                                    \"protocol_number\": protocolNum,\n",
    "                                    \"speaker_name\": name,\n",
    "                                    \"sentence_text\": \" \".join(tokens)\n",
    "                                })\n",
    "                # now we repeat the same process if there was no speaker found by attributing the text to the last speaker\n",
    "                elif lastSpeaker:\n",
    "                    additionalSentences = divideToSentences(text)\n",
    "                    for sentence in additionalSentences:\n",
    "                        if ValidSentence(sentence):\n",
    "                            tokens = Tokenize(sentence)\n",
    "                            if len(tokens) >= 4:\n",
    "                                jsonlList.append({\n",
    "                                    \"protocol_name\": file,\n",
    "                                    \"knesset_number\": knessetNumber,\n",
    "                                    \"protocol_type\": protocolType,\n",
    "                                    \"protocol_number\": protocolNum,\n",
    "                                    \"speaker_name\": lastSpeaker,\n",
    "                                    \"sentence_text\": \" \".join(tokens)\n",
    "                                })\n",
    "        # here we catch the exception if a problem occured while processing a file\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    # here we open the output file in writing mode and we write each item from the list to here\n",
    "    with open(outputFile, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for entry in jsonlList:\n",
    "            jsonl_file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "    # print where the file was stored to see if it worked\n",
    "    print(f\"JSONL file saved at: {outputFile}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # check if the command is wrong\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python processing_knesset_corpus.py <path/to/input_corpus_dir> <path/to/outputFile_name.jsonl>\")\n",
    "        sys.exit(1)\n",
    "    # otherwise we want to take the path for the input and the path for the output\n",
    "    inputFolder = sys.argv[1]\n",
    "    outputFile = sys.argv[2]\n",
    "    # pass the paths to the processing function\n",
    "    workOnFilesFunc(inputFolder, outputFile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
