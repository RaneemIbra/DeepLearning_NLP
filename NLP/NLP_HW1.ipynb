{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['13_ptm_532058.docx', '13_ptm_532066.docx', '13_ptm_532240.docx', '13_ptm_532389.docx', '14_ptm_532484.docx', '14_ptm_532608.docx', '14_ptm_532731.docx', '15_ptm_532756.docx', '15_ptm_532855.docx', '15_ptm_533086.docx', '15_ptv_490845.docx', '15_ptv_490916.docx', '15_ptv_494321.docx', '15_ptv_494780.docx', '15_ptv_495206.docx', '15_ptv_495295.docx', '15_ptv_496915.docx', '15_ptv_496944.docx', '15_ptv_498215.docx', '16_ptm_128954.docx', '16_ptm_128968.docx', '16_ptm_129080.docx', '16_ptm_129137.docx', '16_ptm_129202.docx', '16_ptm_533215.docx', '16_ptm_533607.docx', '16_ptv_386758.docx', '16_ptv_386822.docx', '16_ptv_386833.docx', '16_ptv_489839.docx', '16_ptv_491962.docx', '16_ptv_493376.docx', '16_ptv_499021.docx', '16_ptv_499045.docx', '16_ptv_548123.docx', '16_ptv_548530.docx', '16_ptv_549100.docx', '16_ptv_572718.docx', '16_ptv_577443.docx', '16_ptv_577758.docx', '16_ptv_581836.docx', '16_ptv_71595.docx', '17_ptm_129748.docx', '17_ptm_533398.docx', '17_ptm_533401.docx', '17_ptm_533539.docx', '17_ptv_132014.docx', '17_ptv_132523.docx', '17_ptv_132714.docx', '17_ptv_133151.docx', '17_ptv_133454.docx', '17_ptv_133749.docx', '17_ptv_134006.docx', '17_ptv_134507.docx', '17_ptv_135103.docx', '17_ptv_135556.docx', '17_ptv_136809.docx', '17_ptv_137100.docx', '18_ptv_138917.docx', '18_ptv_139021.docx', '18_ptv_139299.docx', '18_ptv_139592.docx', '18_ptv_139917.docx', '18_ptv_139933.docx', '18_ptv_165084..docx', '18_ptv_165095..docx', '18_ptv_166075..docx', '19_ptm_304163.docx', '19_ptv_232326.docx', '19_ptv_253687.docx', '19_ptv_262672.docx', '19_ptv_266962.docx', '19_ptv_279660.docx', '19_ptv_302840.docx', '20_ptm_313902.docx', '20_ptm_341723.docx', '20_ptm_394541.docx', '20_ptm_487089.docx', '20_ptv_311936.docx', '20_ptv_320584.docx', '20_ptv_341020.docx', '20_ptv_341230.docx', '20_ptv_370910.docx', '20_ptv_387379.docx', '20_ptv_397418.docx', '20_ptv_488037.docx', '20_ptv_490139.docx', '20_ptv_494974.docx', '20_ptv_519812.docx', '23_ptv_578475.docx', '23_ptv_582824.docx', '23_ptv_598323 - Copy.docx', '23_ptv_598323.docx', '23_ptv_598723.docx', '23_ptv_599659.docx', '23_ptv_600338.docx', '25_ptv_1219728.docx', '25_ptv_1457545.docx', '25_ptv_3841247.docx', 'for_test_23_ptv_585004.docx']\n",
      "JSONL file saved at: C:/Users/Alpha/Downloads/knesset_protocols/protocol_for_hw1/protocol_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from docx import Document\n",
    "\n",
    "# function to split text into sentences\n",
    "def divide_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits the input text into sentences based on punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to split into sentences.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        sentence += char\n",
    "        if char in \".!?\":  # sentence-ending punctuation\n",
    "            if len(sentence.strip()) > 1:  # avoid single-character sentences\n",
    "                sentences.append(sentence.strip())\n",
    "                sentence = \"\"\n",
    "    \n",
    "    if sentence.strip():  # add any remaining text as the last sentence\n",
    "        sentences.append(sentence.strip())\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# function to check if a sentence is valid\n",
    "def is_valid_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is valid based on the question criteria.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The sentence to validate.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the sentence is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    # check if the sentence contains Hebrew characters\n",
    "    if not re.search(r\"[\\u0590-\\u05FF]\", sentence):\n",
    "        return False\n",
    "    \n",
    "    # exclude sentences with only special characters\n",
    "    if re.fullmatch(r\"[^\\w\\u0590-\\u05FF]+\", sentence):\n",
    "        return False\n",
    "    \n",
    "    # avoid sentences with placeholder symbols\n",
    "    if re.search(r\"\\.\\.\\.|---\", sentence):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# function to tokenize a sentence into words and symbols\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence into words and punctuation marks.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The sentence to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for word in sentence.split():  # split the sentence into words\n",
    "        # split words into smaller tokens if necessary\n",
    "        tokens.extend(re.findall(r\"\\w+|[^\\w\\s]\", word))\n",
    "    return tokens\n",
    "\n",
    "# main processing script\n",
    "def process_protocol_files(folder_path):\n",
    "    \"\"\"\n",
    "    Processes .docx files in a given folder, extracting and tokenizing Hebrew text.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .docx files.\n",
    "    \"\"\"\n",
    "    # find all .docx files in the folder\n",
    "    protocol_files = [f for f in os.listdir(folder_path) if f.endswith(\".docx\")]\n",
    "    jsonl_data = []\n",
    "\n",
    "    print(\"Found files:\", protocol_files)\n",
    "    \n",
    "    for file in protocol_files:\n",
    "        match = re.search(r'(\\d+)_pt', file)\n",
    "        knesset_number = int(match.group(1)) if match else -1  # extract Knesset number\n",
    "        \n",
    "        # determine the protocol type based on file name\n",
    "        if \"ptm\" in file:\n",
    "            protocol_type = \"plenary\"\n",
    "        elif \"ptv\" in file:\n",
    "            protocol_type = \"committee\"\n",
    "        else:\n",
    "            protocol_type = \"undefined\"\n",
    "        \n",
    "        protocol_number = None\n",
    "        \n",
    "        try:\n",
    "            doc_path = os.path.join(folder_path, file)\n",
    "            doc = Document(doc_path)  # open the document\n",
    "            \n",
    "            # extract the protocol number from the first few paragraphs\n",
    "            for paragraph in doc.paragraphs[:10]:\n",
    "                match = re.search(r\"פרוטוקול מס'? (\\d+)\", paragraph.text)\n",
    "                if match:\n",
    "                    protocol_number = int(match.group(1))\n",
    "                    break\n",
    "            \n",
    "            if protocol_number is None:\n",
    "                protocol_number = -1  # if not found then the number is -1\n",
    "            \n",
    "            last_speaker = None\n",
    "            \n",
    "            # process the document paragraph by paragraph\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text = paragraph.text.strip()  # remove extra whitespace\n",
    "                \n",
    "                if not text:  # skip empty paragraphs\n",
    "                    continue\n",
    "                \n",
    "                # check if the paragraph starts with a speaker name\n",
    "                speaker_match = re.match(r\"^([\\u0590-\\u05FF\\w\\s\\(\\)]+):\", text)\n",
    "                if speaker_match:\n",
    "                    raw_name = speaker_match.group(1)  # extract the speaker's name\n",
    "                    name = re.sub(r\"\\s*\\(.*?\\)\", \"\", raw_name).strip()  # remove the extra info\n",
    "                    spoken_text = text[len(speaker_match.group(0)):].strip()\n",
    "                    last_speaker = name  # Update the last speaker\n",
    "                    \n",
    "                    # split the spoken text into sentences\n",
    "                    sentences = divide_into_sentences(spoken_text)\n",
    "                    \n",
    "                    for sentence in sentences:\n",
    "                        if is_valid_sentence(sentence):\n",
    "                            tokens = tokenize_sentence(sentence)\n",
    "                            if len(tokens) >= 4:  # store only tokens with length more or equals to 4\n",
    "                                jsonl_data.append({\n",
    "                                    \"protocol_name\": file,\n",
    "                                    \"knesset_number\": knesset_number,\n",
    "                                    \"protocol_type\": protocol_type,\n",
    "                                    \"protocol_number\": protocol_number,\n",
    "                                    \"speaker_name\": name,\n",
    "                                    \"sentence_text\": \" \".join(tokens)\n",
    "                                })\n",
    "                elif last_speaker:\n",
    "                    # if no speaker is found, then the text belongs to the last speaker\n",
    "                    additional_sentences = divide_into_sentences(text)\n",
    "                    for sentence in additional_sentences:\n",
    "                        if is_valid_sentence(sentence):\n",
    "                            tokens = tokenize_sentence(sentence)\n",
    "                            if len(tokens) >= 4:\n",
    "                                jsonl_data.append({\n",
    "                                    \"protocol_name\": file,\n",
    "                                    \"knesset_number\": knesset_number,\n",
    "                                    \"protocol_type\": protocol_type,\n",
    "                                    \"protocol_number\": protocol_number,\n",
    "                                    \"speaker_name\": last_speaker,\n",
    "                                    \"sentence_text\": \" \".join(tokens)\n",
    "                                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    \n",
    "    # save the results to a JSONL file\n",
    "    output_file = os.path.join(folder_path, \"protocol_data.jsonl\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for entry in jsonl_data:\n",
    "            jsonl_file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"JSONL file saved at: {output_file}\")\n",
    "\n",
    "# define the folder path and start processing\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/Alpha/Downloads/knesset_protocols/protocol_for_hw1/\"\n",
    "    process_protocol_files(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
