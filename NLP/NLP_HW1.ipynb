{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found files:\n",
      "JSONL file saved at: C:/Users/Alpha/Downloads/knesset_protocols/protocol_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def divide_into_sentences(text):\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "\n",
    "    for char in text:\n",
    "        sentence += char\n",
    "        if char in \".!?:\":\n",
    "            if len(sentence.strip()) > 1:\n",
    "                sentences.append(sentence.strip())\n",
    "                sentence = \"\"\n",
    "\n",
    "    if sentence.strip():\n",
    "        sentences.append(sentence.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def is_valid_sentence(sentence):\n",
    "    if not re.search(r\"[\\u0590-\\u05FF]\", sentence):\n",
    "        return False\n",
    "\n",
    "    if re.fullmatch(r\"[^\\w\\u0590-\\u05FF]+\", sentence):\n",
    "        return False\n",
    "\n",
    "    if re.search(r\"\\.\\.\\.|- - -\", sentence):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    abbreviation_pattern = re.compile(r\"\\b(?:e\\.g\\.|i\\.e\\.|Mr\\.|Mrs\\.|Dr\\.|Prof\\.)\\b\", re.IGNORECASE)\n",
    "    \n",
    "    abbreviations = {}\n",
    "    for match in abbreviation_pattern.findall(sentence):\n",
    "        placeholder = f\"{{abbrev_{len(abbreviations)}}}\"\n",
    "        abbreviations[placeholder] = match\n",
    "        sentence = sentence.replace(match, placeholder)\n",
    "\n",
    "    tokens = []\n",
    "    for word in sentence.split():\n",
    "        tokens.extend(re.findall(r\"\\w+|[^\\w\\s]\", word))\n",
    "\n",
    "    tokens = [abbreviations.get(token, token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "folder_path = \"C:/Users/Alpha/Downloads/knesset_protocols/protocol_for_hw1/\"\n",
    "protocol_files = [f for f in os.listdir(folder_path) if f.endswith(\".docx\")]\n",
    "\n",
    "jsonl_data = []\n",
    "\n",
    "print(\"found files:\")\n",
    "for file in protocol_files:\n",
    "    match = re.search(r'(\\d+)_pt', file)\n",
    "    knesset_number = int(match.group(1)) if match else -1\n",
    "\n",
    "    if \"ptm\" in file:\n",
    "        protocol_type = \"plenary\"\n",
    "    elif \"ptv\" in file:\n",
    "        protocol_type = \"committee\"\n",
    "    else:\n",
    "        protocol_type = \"undefined\"\n",
    "\n",
    "    protocol_number = None \n",
    "\n",
    "    try:\n",
    "        doc_path = os.path.join(folder_path, file)\n",
    "        doc = Document(doc_path)\n",
    "\n",
    "        for paragraph in doc.paragraphs[:10]:\n",
    "            match = re.search(r\"פרוטוקול מס'? (\\d+)\", paragraph.text)\n",
    "            if match:\n",
    "                protocol_number = int(match.group(1))\n",
    "                break\n",
    "        if protocol_number is None:\n",
    "            protocol_number = -1\n",
    "\n",
    "        last_speaker = None\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text = paragraph.text.strip()\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            speaker_match = re.match(r\"^([\\u0590-\\u05FF\\w\\s\\(\\)]+):\", text)\n",
    "            if speaker_match:\n",
    "                raw_name = speaker_match.group(1)\n",
    "                name = re.sub(r\"\\s*\\(.*?\\)\", \"\", raw_name).strip()\n",
    "                name = re.sub(r\"^(Chairman|Dr\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*\", \"\", name)\n",
    "\n",
    "                spoken_text = text[len(speaker_match.group(0)):].strip()\n",
    "                last_speaker = name\n",
    "\n",
    "                sentences = divide_into_sentences(spoken_text)\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    if is_valid_sentence(sentence):\n",
    "                        tokens = tokenize_sentence(sentence)\n",
    "                        if len(tokens) >= 4:\n",
    "                            jsonl_data.append({\n",
    "                                \"protocol_name\": file,\n",
    "                                \"knesset_number\": knesset_number,\n",
    "                                \"protocol_type\": protocol_type,\n",
    "                                \"protocol_number\": protocol_number,\n",
    "                                \"speaker_name\": name,\n",
    "                                \"sentence_text\": \" \".join(tokens)\n",
    "                            })\n",
    "            elif last_speaker:\n",
    "                additional_sentences = divide_into_sentences(text)\n",
    "                for sentence in additional_sentences:\n",
    "                    if is_valid_sentence(sentence):\n",
    "                        tokens = tokenize_sentence(sentence)\n",
    "                        if len(tokens) >= 4:\n",
    "                            jsonl_data.append({\n",
    "                                \"protocol_name\": file,\n",
    "                                \"knesset_number\": knesset_number,\n",
    "                                \"protocol_type\": protocol_type,\n",
    "                                \"protocol_number\": protocol_number,\n",
    "                                \"speaker_name\": last_speaker,\n",
    "                                \"sentence_text\": \" \".join(tokens)\n",
    "                            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "output_file = \"C:/Users/Alpha/Downloads/knesset_protocols/protocol_data.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    for entry in jsonl_data:\n",
    "        jsonl_file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL file saved at: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
